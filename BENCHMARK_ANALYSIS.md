# DeepSeek-V3 基准测试详细解读

明确提出了 DeepSeek-V3 仅使用2048块GPU训练了2个月，并且只花费了557.6万美金。 



## 概述
本文档提供了 DeepSeek-V3 在各项基准测试中的详细表现分析，包括原始数据和深入解读。

## 模型架构对比
| 模型 | 架构 | 总参数量 | 推理时激活参数量 |
|-----|-----|---------|----------------|
| DeepSeek-V3 | MoE | 671B | 37B |
| Qwen2.5 | Dense | 72B | 72B |
| LLaMA3.1 | Dense | 405B | 405B |

关键发现：DeepSeek-V3 采用 MoE 架构，虽然总参数量达到 671B，但推理时仅需激活 37B 参数，展现了极高的架构效率。

## 详细性能分析

### 1. 基础模型评测

#### 英语理解能力
| 测试基准 | 测试方式 | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |
|---------|---------|-------------|--------------|----------------|-------------|
| Pile-test (BPB) | - | 0.606 | 0.638 | 0.542 | **0.548** |
| BBH (EM) | 3-shot | 78.8 | 79.8 | 82.9 | **87.5** |
| MMLU (准确率) | 5-shot | 78.4 | 85.0 | 84.4 | **87.1** |
| MMLU-Redux (准确率) | 5-shot | 75.6 | 83.2 | 81.3 | **86.2** |
| MMLU-Pro (准确率) | 5-shot | 51.4 | 58.3 | 52.8 | **64.4** |
| DROP (F1) | 3-shot | 80.4 | 80.6 | 86.0 | **89.0** |
| ARC-Easy (准确率) | 25-shot | 97.6 | 98.4 | 98.4 | **98.9** |
| ARC-Challenge (准确率) | 25-shot | 92.2 | 94.5 | 95.3 | **95.3** |
| HellaSwag (准确率) | 10-shot | 87.1 | 84.8 | 89.2 | **88.9** |
| PIQA (准确率) | 0-shot | 83.9 | 82.6 | 85.9 | **84.7** |
| WinoGrande (准确率) | 5-shot | **86.3** | 82.3 | 85.2 | 84.9 |
| RACE-Middle (准确率) | 5-shot | 73.1 | 68.1 | **74.2** | 67.1 |
| RACE-High (准确率) | 5-shot | 52.6 | 50.3 | **56.8** | 51.3 |
| TriviaQA (EM) | 5-shot | 80.0 | 71.9 | 82.7 | **82.9** |
| NaturalQuestions (EM) | 5-shot | 38.6 | 33.2 | **41.5** | 40.0 |
| AGIEval (准确率) | 0-shot | 57.5 | 75.8 | 60.6 | **79.6** |

解读：
- MMLU 系列测试中，DeepSeek-V3 全面领先，尤其在专业知识领域（MMLU-Pro）的提升显著
- 在部分阅读理解任务（如 RACE 系列）表现略有不足
- 在通用知识问答（TriviaQA）和推理任务（BBH）中表现优异

#### 代码能力
| 测试基准 | 测试方式 | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |
|---------|---------|-------------|--------------|----------------|-------------|
| HumanEval (Pass@1) | 0-shot | 43.3 | 53.0 | 54.9 | **65.2** |
| MBPP (Pass@1) | 3-shot | 65.0 | 72.6 | 68.4 | **75.4** |
| LiveCodeBench-Base (Pass@1) | 3-shot | 11.6 | 12.9 | 15.5 | **19.4** |
| CRUXEval-I (准确率) | 2-shot | 52.5 | 59.1 | 58.5 | **67.3** |
| CRUXEval-O (准确率) | 2-shot | 49.8 | 59.9 | 59.9 | **69.8** |

解读：
- 在所有代码相关测试中全面领先
- HumanEval 测试提升显著，比第二名高出10个百分点以上
- 在实际编程场景（LiveCodeBench）中表现突出

#### 数学能力
| 测试基准 | 测试方式 | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |
|---------|---------|-------------|--------------|----------------|-------------|
| GSM8K (准确率) | 8-shot | 81.6 | 88.3 | 83.5 | **89.3** |
| MATH (准确率) | 4-shot | 43.4 | 54.4 | 49.0 | **61.6** |
| MGSM (准确率) | 8-shot | 63.6 | 76.2 | 69.9 | **79.8** |
| CMath (准确率) | 3-shot | 78.7 | 84.5 | 77.3 | **90.7** |

解读：
- 在所有数学测试中均达到最佳表现
- 在高等数学问题（MATH）上的提升特别显著
- 中文数学题（CMath）的表现尤为突出，准确率达到90.7%

#### 中文能力
| 测试基准 | 测试方式 | DeepSeek-V2 | Qwen2.5 72B | LLaMA3.1 405B | DeepSeek-V3 |
|---------|---------|-------------|--------------|----------------|-------------|
| CLUEWSC (准确率) | 5-shot | 82.0 | 82.5 | **83.0** | 82.7 |
| C-Eval (准确率) | 5-shot | 81.4 | 89.2 | 72.5 | **90.1** |
| CMMLU (准确率) | 5-shot | 84.0 | **89.5** | 73.7 | 88.8 |
| CMRC (准确率) | 1-shot | **77.4** | 75.8 | 76.0 | 76.3 |
| C3 (准确率) | 0-shot | 77.4 | 76.7 | **79.7** | 78.6 |
| CCPM (准确率) | 0-shot | **93.0** | 88.5 | 78.6 | 92.0 |

解读：
- 在综合性中文评测（C-Eval）中达到最佳表现
- 与专注中文的模型（如 Qwen2.5）相比表现相当
- 在某些中文任务上仍有提升空间

### 2. 对话模型评测

#### 标准基准测试（仅限67B以上大模型）

##### 模型基本信息对比
| 模型信息 | DeepSeek V2-0506 | DeepSeek V2.5-0905 | Qwen2.5 72B | LLaMA3.1 405B | Claude-3.5 | GPT-4 | DeepSeek V3 |
|---------|------------------|-------------------|--------------|----------------|------------|--------|-------------|
| 架构 | MoE | MoE | Dense | Dense | - | - | MoE |
| 激活参数量 | 21B | 21B | 72B | 405B | - | - | 37B |
| 总参数量 | 236B | 236B | 72B | 405B | - | - | 671B |

##### 英语理解能力
| 测试基准 | 测试说明 | DeepSeek V2-0506 | DeepSeek V2.5-0905 | Qwen2.5 72B | LLaMA3.1 405B | Claude-3.5 | GPT-4 | DeepSeek V3 |
|---------|----------|------------------|-------------------|--------------|----------------|------------|--------|-------------|
| MMLU (EM) | 57个学科的专业知识测试 | 78.2 | 80.6 | 85.3 | **88.6** | 88.3 | 87.2 | **88.5** |
| MMLU-Redux (EM) | MMLU改进版，包含现代科技等新领域 | 77.9 | 80.3 | 85.6 | 86.2 | 88.9 | 88.0 | **89.1** |
| MMLU-Pro (EM) | 专业领域深度测试 | 58.5 | 66.2 | 71.6 | 73.3 | **78.0** | 72.6 | 75.9 |
| DROP (3-shot F1) | 阅读理解与数值推理 | 83.0 | 87.8 | 76.7 | 88.7 | 88.3 | 83.7 | **91.6** |
| IF-Eval | 指令遵循能力评估 | 57.7 | 80.6 | 84.1 | 86.0 | **86.5** | 84.3 | 86.1 |
| GPQA-Diamond | 高质量问答评估 | 35.3 | 41.3 | 49.0 | 51.1 | **65.0** | 49.9 | 59.1 |
| SimpleQA | 简单事实性问答 | 9.0 | 10.2 | 9.1 | 17.1 | 28.4 | **38.2** | 24.9 |
| FRAMES | 多轮对话理解能力 | 66.9 | 65.4 | 69.8 | 70.0 | 72.5 | **80.5** | 73.3 |
| LongBench v2 | 长文本理解能力测试 | 31.6 | 35.4 | 39.4 | 36.1 | 41.0 | 48.1 | **48.7** |

解读：
1. **基础知识掌握**
   - MMLU系列测试中表现优异，与顶级闭源模型水平相当或更优
   - MMLU-Redux得分89.1%，超越所有对标模型
   - MMLU-Pro虽未达到最高，但与Claude-3.5的差距已缩小到2.1个百分点

2. **推理与理解能力**
   - DROP测试91.6%的成绩尤为亮眼，领先第二名近3个百分点
   - IF-Eval达到86.1%，与最佳成绩仅差0.4个百分点
   - GPQA-Diamond得分59.1%，虽未达到Claude-3.5水平，但大幅领先其他模型

3. **特殊场景表现**
   - SimpleQA和FRAMES测试相对偏弱，表明在简单事实提取和多轮对话方面还有提升空间
   - LongBench v2测试中达到48.7%，展现了优秀的长文本处理能力

##### 代码能力
| 测试基准 | 测试说明 | DeepSeek V2-0506 | DeepSeek V2.5-0905 | Qwen2.5 72B | LLaMA3.1 405B | Claude-3.5 | GPT-4 | DeepSeek V3 |
|---------|----------|------------------|-------------------|--------------|----------------|------------|--------|-------------|
| HumanEval-Mul | 多语言编程能力测试 | 69.3 | 77.4 | 77.3 | 77.2 | 81.7 | 80.5 | **82.6** |
| LiveCodeBench (COT) | 实时编程（思维链） | 18.8 | 29.2 | 31.1 | 28.4 | 36.3 | 33.4 | **40.5** |
| LiveCodeBench | 实时编程（直接输出） | 20.3 | 28.4 | 28.7 | 30.1 | 32.8 | 34.2 | **37.6** |
| Codeforces | 算法竞赛题目 | 17.5 | 35.6 | 24.8 | 25.3 | 20.3 | 23.6 | **51.6** |
| SWE Verified | 软件工程实践 | - | 22.6 | 23.8 | 24.5 | **50.8** | 38.8 | 42.0 |
| Aider-Edit | 代码编辑能力 | 60.3 | 71.6 | 65.4 | 63.9 | **84.2** | 72.9 | 79.7 |
| Aider-Polyglot | 多语言代码转换 | - | 18.2 | 7.6 | 5.8 | 45.3 | 16.0 | **49.6** |

解读：
1. **基础编程能力**
   - HumanEval-Mul达到82.6%的最佳成绩，证明了强大的多语言编程能力
   - LiveCodeBench两个版本都获得最佳成绩，表明在实际编程场景中的优势

2. **高级编程能力**
   - Codeforces测试中的51.6%远超其他所有模型，展现了卓越的算法设计能力
   - SWE Verified得分42.0%，接近Claude-3.5的水平，说明具备专业软件工程实践能力

3. **代码处理能力**
   - Aider-Edit 79.7%的成绩仅次于Claude-3.5，显示了强大的代码编辑能力
   - Aider-Polyglot测试中的49.6%最高分说明在代码转换方面具有特殊优势

##### 数学能力
| 测试基准 | 测试说明 | DeepSeek V2-0506 | DeepSeek V2.5-0905 | Qwen2.5 72B | LLaMA3.1 405B | Claude-3.5 | GPT-4 | DeepSeek V3 |
|---------|----------|------------------|-------------------|--------------|----------------|------------|--------|-------------|
| AIME 2024 | 美国数学邀请赛题目 | 4.6 | 16.7 | 23.3 | 23.3 | 16.0 | 9.3 | **39.2** |
| MATH-500 | 高等数学综合测试 | 56.3 | 74.7 | 80.0 | 73.8 | 78.3 | 74.6 | **90.2** |
| CNMO 2024 | 中国数学奥赛题目 | 2.8 | 10.8 | 15.9 | 6.8 | 13.1 | 10.8 | **43.2** |

解读：
1. **竞赛级数学能力**
   - AIME 2024成绩39.2%，是其他模型的1.7-4.2倍
   - CNMO 2024得分43.2%，远超第二名15.9%，展现了解决高难度数学问题的能力

2. **系统性数学知识**
   - MATH-500测试90.2%的成绩领先第二名10个百分点以上
   - 从V2到V3的进步特别显著，提升了近34个百分点

##### 中文能力
| 测试基准 | 测试说明 | DeepSeek V2-0506 | DeepSeek V2.5-0905 | Qwen2.5 72B | LLaMA3.1 405B | Claude-3.5 | GPT-4 | DeepSeek V3 |
|---------|----------|------------------|-------------------|--------------|----------------|------------|--------|-------------|
| CLUEWSC | 中文指代消解 | 89.9 | 90.4 | **91.4** | 84.7 | 85.4 | 87.9 | 90.9 |
| C-Eval | 中文综合能力 | 78.6 | 79.5 | 86.1 | 61.5 | 76.7 | 76.0 | **86.5** |
| C-SimpleQA | 中文简单问答 | 48.5 | 54.1 | 48.4 | 50.4 | 51.3 | 59.3 | **64.8** |

解读：
1. **综合中文能力**
   - C-Eval测试86.5%的成绩领先所有对标模型
   - 相比专注中文的Qwen2.5仍保持优势

2. **特定任务表现**
   - CLUEWSC成绩90.9%接近最佳，说明在细粒度语言理解上的进步
   - C-SimpleQA大幅领先，显示了在中文问答方面的显著提升

## 上下文窗口性能
- 支持最大128K tokens的上下文长度
- 在"大海捞针"测试中表现稳定
- 长文本理解能力出色，衰减程度小

## 核心优势
1. **数学和科学能力**
   - 在所有数学相关测试中全面领先
   - 专业知识领域表现突出

2. **代码生成能力**
   - 在所有代码相关基准测试中表现最佳
   - 多语言编程能力突出

3. **跨语言能力**
   - 中英文双语表现优异
   - 多语言任务（MMMLU-non-English）得分达79.4%

4. **推理分析能力**
   - 在需要深度推理的任务中表现出色
   - 专业领域知识掌握程度高

## 需要关注的方面
- 部分阅读理解任务（RACE-High、RACE-Middle）表现略低于预期
- 某些常识推理任务（PIQA、WinoGrande）仍有提升空间

## 结论
DeepSeek-V3 在以下方面展现了显著优势：
- 数学推理与问题解决能力
- 代码生成与理解能力
- 专业领域知识掌握
- 跨语言处理能力

模型采用的 MoE 架构展现了极高的效率，仅需激活 37B 参数就达到了超越其他需要更多计算资源的密集模型的性能。这种架构设计不仅降低了推理成本，也为未来更大规模模型的发展提供了重要参考。 