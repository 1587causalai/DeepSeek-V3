# 预训练

## Data Construction


我没有非常看懂这个数据的组织形式, 但是我感觉一旦你理解了 FIM, 将会更好地理解这个模型架构的创新 Multi-token Prediction。

![20241227192327](https://s2.loli.net/2024/12/27/DAQjyphv34RbKrt.png)

我来详细解释一下这段关于DeepSeek-V3预训练数据构建的内容：

1. 数据构建的主要改进：
   - 相比DeepSeek-V2，增加了数学和编程相关样本的比例
   - 扩大了多语言覆盖范围，不再局限于英文和中文
   - 优化了数据处理流程，在保持数据多样性的同时减少冗余
   - 借鉴了Ding等人(2024)的文档打包方法来保证数据完整性
   - 最终训练语料包含14.8T高质量且多样化的token

2. Fill-in-Middle (FIM)策略的应用：
   - 借鉴了DeepSeekCoder-V2的经验，发现FIM策略不会影响模型的下一个token预测能力
   - 采用了Prefix-Suffix-Middle (PSM)框架来组织数据
   - 数据结构格式为：`<|fim_begin|>前缀<|fim_hole|>后缀<|fim_end|>中间文本<|eos_token|>`
   - FIM策略的应用比例为0.1，这与PSM框架保持一致

3. 分词器(Tokenizer)的改进：
   - 使用字节级BPE(Byte-level BPE)技术，来自Shibata等人1999年的工作
   - 扩充词表至128K个token
   - 针对多语言压缩效率优化了预分词器和训练数据
   - 新增了组合标点符号和换行符的token
   - 为了解决token边界偏差问题(Lundberg, 2023提出)：
     - 这个问题主要出现在处理没有终端换行符的多行prompt时，特别是在少样本评估场景
     - 解决方案是在训练时随机拆分一定比例的组合token，让模型接触更多特殊情况从而缓解偏差

这些改进体现了DeepSeek团队在预训练数据构建上的几个重要考虑：
1. 数据质量和多样性的平衡
2. 多语言能力的增强
3. 针对特定任务（如数学和编程）的优化
4. 对模型训练中可能出现的技术问题（如token边界偏差）的预见性解决

这套数据处理方案显示了团队在大模型训练数据构建方面的深入思考和系统性方法。需要注意的是，14.8T的token规模也是相当可观的，这表明了训练数据的丰富程度。


## Hyper-parameters


好的,我来用简单的语言解释一下 DeepSeek-V3 这个模型的架构:

这个模型最核心的特点是:

1. 它是一个超大规模的模型,总共有 671B (6710亿)个参数,但每次实际运行时只会用到其中的 37B 参数。这就像是一个巨大的专家团队,每次只调用其中最相关的专家来解决问题。

2. 模型使用了一种叫 MoE (混合专家系统)的技术。简单来说就是把模型分成了 257 个"专家"(1个共享专家 + 256个特殊专家)。每次处理一个词时,只会启动8个最合适的专家,而且这8个专家会被分配到最多4台机器上运行,这样可以提高效率。

3. 在注意力机制方面(也就是模型理解上下文的方式),它用了 128 个"注意力头",每个头都可以关注不同的信息特征。同时它还用了一些压缩技术来提高效率。

4. 它还有一个特殊的功能:在预测下一个词的同时,还会预测再下一个词。这有点像人在阅读时不仅知道下一个词可能是什么,还能大致猜到再下一个词。

总的来说,这个架构的设计理念就是:虽然模型总体很大,但通过各种聪明的设计(比如专家系统、压缩技术等),让它能够高效运行,既保证了强大的能力,又考虑到了实际应用中的效率问题。

这就像是一个超大的图书馆,虽然有成千上万本书,但当你需要查找特定信息时,会有一个智能系统帮你快速定位到最相关的几本书,而不是翻遍整个图书馆。


我来创建一个表格,更准确地对应英文原文的技术参数和架构细节:

| 组件/参数 | 具体配置 | 说明 |
|---------|----------|------|
| Transformer 层数 | 61 层 | 模型的基础层数 |
| 隐藏维度 | 7168 | 模型的基础维度大小 |
| 参数初始化 | 标准差 0.006 | 所有可学习参数的随机初始化方式 |
| MLA 配置 | - 注意力头数(𝑛ℎ): 128<br>- 每个头的维度(𝑑ℎ): 128<br>- KV压缩维度(𝑑𝑐): 512<br>- 查询压缩维度(𝑑′𝑐): 1536<br>- 解耦查询和键的每头维度(𝑑𝑅ℎ): 64 | 多层注意力机制的详细配置 |
| MoE 结构 | - 1个共享专家<br>- 256个路由专家<br>- 每个专家中间隐藏维度: 2048<br>- 每个token激活8个专家<br>- 每个token最多分配到4个节点 | 除了前3层外的所有FFN都被替换为MoE层 |
| 多token预测 | 深度(D) = 1 | 除了预测下一个token外,每个token还会预测额外的一个token |
| 额外优化 | - 压缩潜在向量后添加RMSNorm层<br>- 在宽度瓶颈处乘以额外的缩放因子 | 与DeepSeek-V2相同的优化策略 |
| 总体规模 | - 总参数量: 671B<br>- 每个token激活参数量: 37B | 模型的整体规模和实际运行时的参数量 |




## Long Context Extension


想象一下训练过程就像教一个学生阅读理解：

1. 基础训练：
- 最开始模型只能处理4000个字的文本（就像学生刚开始只能读短文章）
- 使用了一个叫 YaRN 的技术，这个技术主要是改变了模型"看"文本的方式（就像教学生一种新的阅读方法）

2. 第一阶段训练（4K → 32K）：
- 用更长的文本（32K）来训练
- 每批次可以同时训练1920个样本
- 用相对温和的学习速度（7.3×10^-6）
- 训练1000步

3. 第二阶段训练（32K → 128K）：
- 用超长的文本（128K）来训练
- 因为文本变长了，每批次只能训练480个样本
- 保持相同的学习速度
- 也是训练1000步

特别的地方是：
1. 整个过程是渐进式的，不是一步到位
2. YaRN 技术只用在了特定的地方（模型的"记忆"部分）
3. 训练步数不多，说明这更像是一个适应过程，而不是完全重新学习

这就像是先让学生适应读长篇课文，然后再慢慢过渡到读整本书，每个阶段都用相同的阅读方法，只是材料越来越长。


## Evaluation

Ablation Studies for Multi-Token Prediction 非常有意思发现效果的提升，很大程度上就是这个目标函数的改造. 


![20241227193535](https://s2.loli.net/2024/12/27/GeRw1EgfmZaIqzr.png)

![20241227193658](https://s2.loli.net/2024/12/27/eVxmNS4AYpTbz3k.png)