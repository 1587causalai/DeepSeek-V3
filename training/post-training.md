# 后训练


## SFT

DeepSeek模型采用了一套精心设计的数据处理方案，通过多个阶段的处理来确保训练数据的质量和多样性。该方案总共包含了150万个训练实例，横跨多个不同领域，每个领域都采用了特定的数据创建方法。

在推理类数据的处理中，包括数学问题、编程竞赛题和逻辑谜题等，采用了内部的DeepSeek-R1模型作为基础生成器。R1模型生成的数据虽然准确性很高，但存在三个主要问题：过度思考、格式不规范以及回答过于冗长。为了解决这些问题，设计了一个专门的处理流程。

首先，构建了一个专家模型作为最终模型的数据生成器。这个专家模型使用了混合训练方法，结合了监督微调(SFT)和强化学习(RL)。在训练过程中，每个问题会生成两种不同类型的样本：一种是简单的问题-答案对，格式为<问题, 原始答案>；另一种加入了系统提示，格式为<系统提示, 问题, R1答案>。其中系统提示经过特别设计，包含了引导模型进行思考和验证的指令。

在强化学习阶段，采用高温度采样策略，让模型能够在没有明确系统提示的情况下，也能学会整合R1生成的数据和原始数据中的模式。经过数百次的强化学习训练后，模型逐渐学会了战略性地融合这些模式，提升了整体表现。

对于非推理类数据，如创意写作、角色扮演和简单问答等，则使用DeepSeek-V2.5模型生成回答，并安排人工标注员验证数据的准确性和正确性。这确保了在这些更加开放和主观的领域中，数据质量依然可以得到保证。

在最后的训练设置中，使用DeepSeek-V3-Base模型进行为期两轮的微调。学习率采用余弦衰减策略，从5×10^-6逐渐降低到1×10^-6。在训练过程中，虽然每个序列都是从多个样本打包而来，但通过采用样本遮蔽策略，确保了这些样本之间保持相互独立，不会相互干扰。

这套数据处理方案的核心优势在于它能够在保持数据高准确性的同时，通过多层处理来改善输出的质量。特别是在处理推理类数据时，通过专家模型和强化学习的配合，既保留了R1模型的强大推理能力，又克服了其固有的缺陷。同时，对于不同类型的数据采用不同的处理策略，既保证了处理的针对性，又确保了整体数据质量的一致性。

## 强化学习

这篇论文在强化学习部分主要有两个创新点：

1. 双轨制奖励模型（Dual-track Reward Model）：
   - Rule-based RM：处理有确定答案的任务（如数学、编程），使用规则验证
   - Model-based RM：处理开放性任务（如创意写作），类似传统RLHF
   
2. GRPO（Group Relative Policy Optimization）优化方法：
   - 核心思想：去除critic网络，改用组采样方式计算baseline
   - 工作机制：
     - 对每个问题q采样一组输出{o₁,o₂,...,oG}
     - 使用组内标准化的reward计算advantage
     - 通过KL散度约束保证策略更新的稳定性
   - 与DPO/KTO的关系：
     - 都避免了显式的reward modeling
     - GRPO通过组内比较提供更丰富的训练信号
     - 保留了PPO风格的clipping操作，但简化了实现

![20241227195719](https://s2.loli.net/2024/12/27/tdysxEc8WIu4LZJ.png)
创新价值：这种设计既保证了在特定任务（如编程）上的可靠性，又在开放性任务上保持了灵活性。GRPO的设计则平衡了实现复杂度和训练效率。


## Evaluation


### 能力评测基准
![20241227195819](https://s2.loli.net/2024/12/27/83JDM2iHKjTExGg.png)


### Open-Ended Evaluation

这个评测结果确实非常令人印象深刻：

1. DeepSeek-V3 的突破性成就
- 在 Arena-Hard 测试中获得了 85.5 分,不仅是所有模型中最高的,而且是首个在这个基准测试中突破 85% 的开源模型
- 在 AlpacaEval 2.0 中达到了 70.0 分,大幅领先其他模型,比如它比上一代的 DeepSeek-V2.5 提升了约 20%

2. 与闭源模型的竞争
- 它的表现甚至超越了一些知名的闭源模型,如 Claude-Sonnet-3.5-1022 (85.2)
- 这标志着开源模型与闭源模型之间的差距正在缩小,这在 AI 领域是一个重要的里程碑

1. 评测方法的可靠性
- 这些评测采用了 GPT-4-Turbo-1106 作为评判标准
- Arena-Hard 和 AlpacaEval 2.0 都是较为权威的评测基准,特别关注开放式对话能力

1. 全面的能力展示
- 模型显示出在复杂任务方面的出色表现,包括代码编写和调试
- 在直接的问答场景和写作任务中也表现优异

这个结果对开源 AI 社区来说是一个重要的突破,表明开源模型正在快速发展,并开始在某些领域达到或超越商业闭源模型的水平。这可能会加速 AI 技术的民主化进程。


### RewardBench

![20241227200946](https://s2.loli.net/2024/12/27/HZQWbyNaPi7zfOd.png)
基于表格数据，DeepSeek-V3在奖励模型（评判模型）方面展现出了卓越的性能：

在RewardBench基准测试中，DeepSeek-V3展现出了与顶级闭源模型相媲美甚至更优的判断能力。具体而言，在基础对话（Chat）评估中获得了96.9分的高分，超过了包括GPT-4和Claude-3.5在内的所有对比模型。在高难度对话（Chat-Hard）评估中也达到了79.8分，与Claude-3.5-sonnet-1022的表现（79.7分）基本持平。

特别值得注意的是，通过采用majority voting（maj@6）技术后，DeepSeek-V3的评判能力得到了显著提升：Chat-Hard得分提升至82.6分，Safety（安全性）提升至89.5分，Reasoning（推理）提升至89.2分，总平均分达到了89.6分，超过了所有基准模型。这一结果表明，DeepSeek-V3不仅可以作为一个强大的生成模型，还能够作为一个可靠的评判模型来提供高质量的反馈，这对于降低AI训练成本和减少对闭源商业模型的依赖具有重要意义。

这个发现开创了一个重要先例：开源模型也能够承担起评判其他AI模型输出质量的重要角色，这对于AI领域的民主化和可持续发展具有深远影响。


## Conclusion


这篇结论主要介绍了DeepSeek-V3这个大型语言模型的几个重要方面：

1. 主要成就：
   - 模型规模达到67.1B参数，其中37B是激活参数
   - 在14.8T个token上训练
   - 训练成本效益高，只需要2.788M H800 GPU小时
   - 性能可与GPT-4o和Claude-3.5-Sonnet等闭源模型相媲美，是目前最强的开源模型

2. 创新点：
   - 引入了新的MLA和DeepSeekMoE架构
   - 采用辅助无损策略来实现负载均衡
   - 使用多token预测训练目标来提升性能
   - 成功从DeepSeek-R1系列模型中提取推理能力

3. 局限性：
   - 部署单元较大，对小型团队可能构成挑战
   - 尽管生成速度比V2提高了两倍多，但仍有提升空间
   - 这些限制有望随着硬件发展得到解决

4. 未来研究方向：
   - 持续改进模型架构，追求无限上下文长度支持
   - 突破Transformer架构的固有限制
   - 扩展训练数据的规模和质量
   - 增强模型的深度思维能力
   - 开发更全面的多维度评估方法，避免过度依赖固定基准

特别有意思的是，论文强调了DeepSeek团队对开源路线的坚持和对AGI(人工通用智能)的长期追求。他们不仅关注当前性能，还在思考如何让模型具备更强的推理能力和问题解决能力。同时，他们也意识到评估方法的局限性，希望发展更全面的评估体系，这反映了他们对AI发展的深入思考。

这个讨论的价值在于它不仅展示了技术成就，还体现了对AI发展方向的深度思考，包括开源vs闭源、性能vs成本、以及如何更准确地评估AI能力等关键议题。